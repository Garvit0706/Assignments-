{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf642979-3a87-4995-89da-f7bf142d924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What is the Filter method in feature selection, and how does it work?\n",
    "Filter Method:\n",
    "\n",
    "Definition: The filter method is a feature selection technique that uses statistical measures to select the most relevant features from the dataset.\n",
    "How it Works:\n",
    "Step 1: Each feature is ranked based on a statistical score.\n",
    "Step 2: Features are selected or discarded based on their scores.\n",
    "Statistical Measures:\n",
    "Correlation Coefficient: Measures the linear relationship between the feature and the target variable.\n",
    "Chi-Square Test: Measures the independence of the feature with the target variable.\n",
    "Mutual Information: Measures the amount of information gained about the target variable given the feature.\n",
    "Variance Threshold: Removes features with low variance.\n",
    "Example:\n",
    "    from sklearn.feature_selection import SelectKBest, chi2\n",
    "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48481ef-722e-4942-ab79-9a09be8cff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: How does the Wrapper method differ from the Filter method in feature selection?\n",
    "Wrapper Method:\n",
    "\n",
    "Definition: The wrapper method evaluates feature subsets based on their predictive power using a specific model.\n",
    "How it Works:\n",
    "Step 1: Generate subsets of features.\n",
    "Step 2: Train a model using each subset and evaluate its performance.\n",
    "Step 3: Select the subset with the best performance.\n",
    "Techniques:\n",
    "Forward Selection: Starts with no features, adds features one by one, and evaluates the model performance.\n",
    "Backward Elimination: Starts with all features, removes features one by one, and evaluates the model performance.\n",
    "Recursive Feature Elimination (RFE): Iteratively builds a model, ranks features by importance, and removes the least important features.\n",
    "Difference:\n",
    "\n",
    "Filter Method: Uses statistical measures independently of the model, faster, less computationally intensive.\n",
    "Wrapper Method: Uses model performance as a measure, slower, more computationally intensive but potentially more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4a5ecd-1b79-4c98-8798-d2dfa650e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3: What are some common techniques used in Embedded feature selection methods?\n",
    "Embedded Methods:\n",
    "\n",
    "Definition: Embedded methods perform feature selection during the model training process.\n",
    "Techniques:\n",
    "Lasso Regression (L1 Regularization): Adds a penalty equal to the absolute value of the magnitude of coefficients, resulting in some coefficients being zero and effectively selecting features.\n",
    "Ridge Regression (L2 Regularization): Adds a penalty equal to the square of the magnitude of coefficients, reduces coefficients but doesnâ€™t necessarily eliminate features.\n",
    "Elastic Net: Combines L1 and L2 regularization.\n",
    "Tree-Based Methods: Decision trees and ensemble methods like Random Forest and Gradient Boosting provide feature importance scores based on the contribution of each feature to the prediction.\n",
    "Example:\n",
    "    from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "importance = np.abs(lasso.coef_)\n",
    "selected_features = X.columns[importance > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa81cabf-bcd8-441b-9dcf-d4ef37e6857d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What are some drawbacks of using the Filter method for feature selection?\n",
    "Drawbacks:\n",
    "\n",
    "Independence Assumption: Assumes features are independent of each other, which is often not true.\n",
    "Model Agnostic: Does not consider the specific learning algorithm used, might not select features that work best with the given model.\n",
    "Univariate: Often considers each feature in isolation, missing interactions between features.\n",
    "Oversimplification: Might oversimplify the feature selection process, leading to suboptimal feature sets for complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776bece-712c-4f0b-884d-8889c76436ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: In which situations would you prefer using the Filter method over the Wrapper method for feature selection?\n",
    "Preferred Situations for Filter Method:\n",
    "\n",
    "High-Dimensional Data: When the dataset has a large number of features, the filter method is computationally efficient.\n",
    "Preliminary Analysis: When a quick assessment of feature relevance is needed before applying more sophisticated methods.\n",
    "Baseline Model: When creating a baseline model to compare with more complex models.\n",
    "Low Computational Resources: When computational resources or time are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a7cc4c-93ef-43ee-847e-2a6358b41339",
   "metadata": {},
   "source": [
    "Q6: Using the Filter Method to Select Features for Predictive Model in a Telecom Company\n",
    "Steps:\n",
    "\n",
    "Correlation Analysis: Compute the correlation between each feature and the target variable (customer churn). Select features with high correlation.\n",
    "import pandas as pd\n",
    "df = pd.read_csv('telecom_data.csv')\n",
    "correlation = df.corr()\n",
    "relevant_features = correlation['churn'].sort_values(ascending=False).index[:10]\n",
    "df_filtered = df[relevant_features]\n",
    "\n",
    "Chi-Square Test: Use the chi-square test for categorical features.\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "X = df.drop('churn', axis=1)\n",
    "y = df['churn']\n",
    "X_new = SelectKBest(chi2, k=10).fit_transform(X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375c6fd-eb4f-4011-b253-a32c16841161",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7: Using Embedded Method for Soccer Match Outcome Prediction\n",
    "Steps:\n",
    "\n",
    "Train a Tree-Based Model: Use a tree-based model like Random Forest or Gradient Boosting\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "Feature Importance: Extract feature importances from the trained model.\n",
    "importances = model.feature_importances_\n",
    "selected_features = X.columns[np.argsort(importances)[-10:]]\n",
    "\n",
    "Lasso Regularization: Apply Lasso to select relevant features\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "selected_features = X.columns[np.abs(lasso.coef_) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1a0d5-69fd-4b7e-8258-42869173ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: Using Wrapper Method for House Price Prediction\n",
    "Steps:\n",
    "\n",
    "Forward Selection: Start with no features and iteratively add features that improve model performance.\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "sfs = SFS(LinearRegression(), \n",
    "          k_features='best', \n",
    "          forward=True, \n",
    "          floating=False, \n",
    "          scoring='r2', \n",
    "          cv=5)\n",
    "sfs = sfs.fit(X, y)\n",
    "selected_features = list(sfs.k_feature_names_)\n",
    "\n",
    "\n",
    "Backward Elimination: Start with all features and iteratively remove the least significant features.\n",
    "sfs = SFS(LinearRegression(), \n",
    "          k_features='best', \n",
    "          forward=False, \n",
    "          floating=False, \n",
    "          scoring='r2', \n",
    "          cv=5)\n",
    "sfs = sfs.fit(X, y)\n",
    "selected_features = list(sfs.k_feature_names_)\n",
    "\n",
    "\n",
    "Recursive Feature Elimination (RFE): Use RFE with a model to rank features by importance and recursively eliminate the least important features.\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=LinearRegression(), n_features_to_select=5, step=1)\n",
    "rfe = rfe.fit(X, y)\n",
    "selected_features = X.columns[rfe.support_]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
