{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a7227-3d5f-49a3-8beb-e15ede0afa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "Min-Max Scaling:\n",
    "\n",
    "Definition: Min-Max scaling, also known as normalization, transforms features to a fixed range, usually [0, 1] or [-1, 1].\n",
    "Usage: This scaling is useful when the dataset does not follow a Gaussian distribution and when the values need to be scaled proportionally.\n",
    "Example:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04e7680-34c4-49f6-8750-09f77df6cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2: What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "Unit Vector Scaling:\n",
    "\n",
    "Definition: Scales the feature vector to a unit norm, i.e., the Euclidean length of the vector becomes 1.\n",
    "Difference from Min-Max Scaling: While Min-Max scaling adjusts the values within a fixed range, \n",
    "Unit Vector scaling normalizes the length of the feature vector.\n",
    "Example:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "data = np.array([[4, 1, 2, 2]])\n",
    "scaler = Normalizer(norm='l2')\n",
    "scaled_data = scaler.transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a6212-b910-409a-bc3e-451f35cfa120",
   "metadata": {},
   "source": [
    "Q3: What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "PCA (Principal Component Analysis):\n",
    "\n",
    "Definition: PCA is a technique used to emphasize variation and bring out strong patterns in a dataset by transforming the data into a new coordinate system where the greatest variances come to lie on the first few coordinates (principal components).\n",
    "Usage: Used for dimensionality reduction, noise reduction, and data visualization.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n",
    "pca = PCA(n_components=1)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "print(reduced_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6b604-0bb1-475f-9931-7b841b316a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4: What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? \n",
    "Provide an example to illustrate this concept.\n",
    "Relationship:\n",
    "\n",
    "Feature Extraction: PCA transforms the original features into a new set of features (principal components) that are uncorrelated and \n",
    "capture the maximum variance.\n",
    "Usage: PCA can be used to extract the most important features from a dataset, reducing the dimensionality while retaining most of the\n",
    "original information.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = np.array([[2.5, 2.4], [0.5, 0.7], [2.2, 2.9], [1.9, 2.2], [3.1, 3.0], [2.3, 2.7], [2, 1.6], [1, 1.1], [1.5, 1.6], [1.1, 0.9]])\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc2b32-24f3-4eb2-a138-4d1c72180ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5: Using Min-Max Scaling to Preprocess Data for a Recommendation System\n",
    "Steps:\n",
    "\n",
    "Load Data: Load the dataset containing features such as price, rating, and delivery time.\n",
    "Apply Min-Max Scaling: Scale each feature to the range [0, 1] or [-1, 1] as required.\n",
    "Example:\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([[20, 4.5, 30], [10, 3.0, 45], [15, 5.0, 20]])\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cb3e22-9141-4727-941f-3945abeff1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6: Using PCA to Reduce Dimensionality for Stock Price Prediction\n",
    "Steps:\n",
    "\n",
    "Load Data: Load the dataset containing various features.\n",
    "Apply PCA: Reduce the dimensionality while retaining the most important information.\n",
    "Example:\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = np.random.rand(100, 20)  # Simulated stock price data with 20 features\n",
    "pca = PCA(n_components=5)\n",
    "reduced_data = pca.fit_transform(data)\n",
    "print(reduced_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6488b9b3-8b47-4a97-84fd-d78715fb4387",
   "metadata": {},
   "outputs": [],
   "source": [
    "7.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([[1], [5], [10], [15], [20]])\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c772c-92e8-4825-835b-810b965b09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8: Perform Feature Extraction Using PCA on [height, weight, age, gender, blood pressure]\n",
    "Steps:\n",
    "\n",
    "Load Data: Simulate or load the dataset.\n",
    "Apply PCA: Determine the number of principal components based on explained variance.\n",
    "Example:\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Simulate dataset with 5 features\n",
    "data = np.random.rand(100, 5)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.components_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
