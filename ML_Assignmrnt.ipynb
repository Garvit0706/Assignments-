{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74a0e14-0e77-4d33-b133-3196d5a4b303",
   "metadata": {},
   "source": [
    "Q1: Overfitting occurs when a machine learning model fits the training data too closely, resulting in poor performance on new, unseen data. On the other hand, underfitting occurs when the model is too simple and does not capture the underlying patterns in the data, resulting in poor performance on both training and test data. Overfitting leads to high variance, while underfitting leads to high bias. Regularization techniques, such as adding a penalty term to the loss function or using dropout, can help mitigate overfitting, while increasing model complexity or using more features can help mitigate underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675ecf91-6163-41c9-8eb0-61c7363b159e",
   "metadata": {},
   "source": [
    "Q2: To reduce overfitting, one can use regularization techniques such as L1 and L2 regularization, dropout, and early stopping. L1 and L2 regularization add a penalty term to the loss function that encourages the model to have small weights, while dropout randomly drops out some neurons during training to prevent them from being too dependent on each other. Early stopping stops training when the model performance on a validation set starts to decrease, indicating that it is starting to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015a475-7d6c-4815-bd43-6882ddc856dc",
   "metadata": {},
   "source": [
    "Q3: Underfitting occurs when a model is too simple to capture the underlying patterns in the data. This can occur when there are too few features, when the model is too simple, or when the data is too noisy or complex. For example, a linear regression model may underfit a dataset that has a nonlinear relationship between the input and output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ba5d6-b594-4f3a-b76d-a5e18c37a49d",
   "metadata": {},
   "source": [
    "Q4: The bias-variance tradeoff is a fundamental concept in machine learning that describes the tradeoff between a model's ability to fit the training data (bias) and its ability to generalize to new, unseen data (variance). A model with high bias is too simple and does not capture the underlying patterns in the data, while a model with high variance is too complex and overfits the training data. To achieve good performance, the model must balance bias and variance by finding the optimal level of complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12574b-22b4-467f-aa00-29dd97dccab9",
   "metadata": {},
   "source": [
    "Q5: Common methods for detecting overfitting and underfitting include cross-validation, learning curves, and visual inspection of the model's performance on a validation set. To determine whether a model is overfitting or underfitting, one can compare the performance on the training and test sets. If the training accuracy is much higher than the test accuracy, the model may be overfitting, while if both accuracies are low, the model may be underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc048cdc-9e9a-4ff5-9d72-b9a9fe7f4cf1",
   "metadata": {},
   "source": [
    "Q6: Bias and variance are two important sources of error in machine learning models. A model with high bias is too simple and does not capture the underlying patterns in the data, while a model with high variance is too complex and overfits the training data. An example of a high bias model is a linear regression model that is too simple to capture a nonlinear relationship in the data, while an example of a high variance model is a decision tree with many levels that overfits the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03945333-fec5-4cc6-b32f-4cd505da734d",
   "metadata": {},
   "source": [
    "Q7: Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function that encourages the model to have small weights. Common regularization techniques include L1 and L2 regularization, dropout, and early stopping. L1 and L2 regularization add a penalty term to the loss function that encourages the model to have small weights, while dropout randomly drops out some neurons during training to prevent them from being too dependent on each other. Early stopping stops training when the model performance on a validation set starts to decrease, indicating that it is starting to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812da49-e148-494e-9373-1ea117121149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
