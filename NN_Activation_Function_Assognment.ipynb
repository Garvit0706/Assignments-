{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50c97351-3d45-4276-9865-292bb14f9726",
   "metadata": {},
   "source": [
    "Q1. An activation function in the context of artificial neural networks is a mathematical function that determines the output of a neuron or node based on its input. It introduces non-linearity into the network, allowing it to model complex relationships between inputs and outputs. Activation functions play a crucial role in enabling neural networks to learn and approximate a wide range of functions.\n",
    "\n",
    "Q2. Some common types of activation functions used in neural networks are:\n",
    "a. Sigmoid: Maps input to a range between 0 and 1.\n",
    "b. Hyperbolic Tangent (tanh): Maps input to a range between -1 and 1.\n",
    "c. Rectified Linear Unit (ReLU): Outputs the input if it's positive, and 0 otherwise.\n",
    "d. Leaky ReLU: Similar to ReLU but allows a small gradient for negative inputs.\n",
    "e. Parametric ReLU (PReLU): Generalization of Leaky ReLU where the slope is learned.\n",
    "f. Exponential Linear Unit (ELU): Similar to ReLU but smooth and allows negative values.\n",
    "g. Swish: Smooth and non-monotonic activation function that performs well in some cases.\n",
    "\n",
    "Q3. Activation functions affect the training process and performance of a neural network in several ways:\n",
    "\n",
    "They introduce non-linearity, allowing the network to learn complex patterns and relationships.\n",
    "They impact the convergence speed of training and can help mitigate issues like vanishing gradients.\n",
    "The choice of activation function can influence the model's expressiveness and ability to handle different types of data.\n",
    "They can address problems like exploding gradients or dying neurons, which can hinder training.\n",
    "\n",
    "Q4. The sigmoid activation function works by mapping the input to a range between 0 and 1. It is defined as f(x) = 1 / (1 + exp(-x)). Its advantages include producing outputs in a interpretable probability-like range, but it has disadvantages like vanishing gradients, making it less suitable for deep networks.\n",
    "\n",
    "Q5. The Rectified Linear Unit (ReLU) activation function works by outputting the input directly if it's positive, and 0 if it's negative. It differs from the sigmoid function by being piecewise linear and not suffering from vanishing gradients for positive inputs.\n",
    "\n",
    "Q6. The benefits of using the ReLU activation function over the sigmoid function are:\n",
    "\n",
    "Reduced vanishing gradient problem, which aids in training deep networks.\n",
    "Simplicity and computational efficiency.\n",
    "Capturing non-linearity effectively in most cases.\n",
    "Promoting sparsity in activations, which can be useful for regularization.\n",
    "\n",
    "Q7. \"Leaky ReLU\" is a variation of the ReLU activation function. It introduces a small slope (typically a small positive constant) for negative inputs. This addresses the vanishing gradient problem by allowing a gradient to flow even for negative values, preventing neurons from becoming inactive during training.\n",
    "\n",
    "Q8. The softmax activation function is used in multi-class classification problems. It converts a vector of raw scores (logits) into a probability distribution over multiple classes, ensuring that the class probabilities sum to 1. It is commonly used in the output layer of neural networks for tasks like image classification and natural language processing.\n",
    "\n",
    "Q9. The hyperbolic tangent (tanh) activation function is similar to the sigmoid function but maps inputs to a range between -1 and 1. It is defined as f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x)). Compared to the sigmoid function, tanh has the advantage of outputting values centered around 0, which can help mitigate vanishing gradients. However, it still suffers from vanishing gradients for extremely large positive or negative inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
