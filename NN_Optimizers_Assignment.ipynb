{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3e54b4-18aa-41ae-8b8c-be4f7b42f4db",
   "metadata": {},
   "source": [
    "1. Role of Optimization Algorithms:\n",
    "Optimization algorithms play a vital role in artificial neural networks by iteratively updating model parameters to minimize a loss function. They are necessary to find the optimal set of weights and biases that lead to the best model performance. Optimization algorithms automate the process of training neural networks and are crucial for improving convergence and model accuracy.\n",
    "\n",
    "2. Gradient Descent and Variants:\n",
    "\n",
    "Gradient Descent (GD) is a fundamental optimization algorithm used in neural networks. It updates parameters by moving in the direction of the steepest descent of the loss function using gradients.\n",
    "Variants: Stochastic Gradient Descent (SGD), Mini-Batch Gradient Descent, and Batch Gradient Descent differ in terms of the data they use for updates (single sample, mini-batch, or entire dataset). Mini-batch GD is commonly used for better convergence and efficiency.\n",
    "\n",
    "3. Challenges of Traditional Gradient Descent:\n",
    "Slow Convergence: Traditional GD can converge slowly, especially in deep networks, requiring many epochs to reach an optimal solution.\n",
    "Local Minima: It can get stuck in local minima or saddle points, hindering convergence to the global minimum.\n",
    "\n",
    "4. Momentum and Learning Rate:\n",
    " Momentum is a concept in optimization algorithms that helps prevent oscillations in the search for the minimum. It accumulates the past gradients to smoothen parameter updates, making convergence faster.\n",
    "The learning rate is a hyperparameter that controls the step size in parameter updates. It significantly impacts convergence and model performance. An appropriate learning rate is crucial for effective training.\n",
    "\n",
    "5. Stochastic Gradient Descent (SGD):\n",
    "\n",
    "Advantages: SGD updates parameters more frequently, which can lead to faster convergence and the ability to escape local minima. It can be more computationally efficient.\n",
    "Limitations: It introduces high variance in parameter updates, which may lead to noisy convergence. It requires fine-tuning of the learning rate.\n",
    "\n",
    "6. Adam Optimizer:\n",
    "The Adam optimizer combines the benefits of momentum and adaptive learning rates. It maintains two moving averages for each parameter: the first moment (mean) and the second moment (uncentered variance).\n",
    "Benefits: Adam provides good convergence speed, works well with noisy gradients, and automatically adapts learning rates. It is widely used and often a reliable choice.\n",
    "Drawbacks: In some cases, Adam may require more memory than traditional GD and might need careful tuning of hyperparameters.\n",
    "\n",
    "7. RMSprop Optimizer:\n",
    "RMSprop is an adaptive learning rate optimizer that divides the learning rate by a running average of the square of past gradients for each parameter.\n",
    "Strengths: It addresses the challenge of adaptive learning rates. RMSprop is memory-efficient and often converges faster than traditional GD.\n",
    "Weaknesses: RMSprop does not have the momentum component of Adam, which can sometimes result in slower convergence on certain tasks compared to Adam.\n",
    "\n",
    "9. Considerations and Tradeoffs:\n",
    "\n",
    "SGD is a simple optimizer, but it may require more fine-tuning of the learning rate. It can be suitable for stable convergence on various tasks.\n",
    "Adam is a popular choice with adaptive learning rates and momentum, providing good convergence speed and stability in many cases.\n",
    "RMSprop is memory-efficient and addresses adaptive learning rates. It can be a faster alternative to Adam on some tasks.\n",
    "The choice of optimizer depends on factors like the task, dataset size, model architecture, and available computational resources. You may need to experiment with different optimizers and hyperparameters to find the most suitable one for your specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211a0f4b-0250-474d-9da5-f18e29063f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mnist\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0  # Normalize pixel values\n",
    "\n",
    "# Build a simple feedforward neural network\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model with different optimizers\n",
    "optimizers = ['SGD', 'Adam', 'RMSprop']\n",
    "results = []\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels), verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "    results.append((optimizer, test_accuracy))\n",
    "\n",
    "# Compare the results\n",
    "for optimizer, accuracy in results:\n",
    "    print(f'Optimizer: {optimizer}, Test accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Plot the training curves\n",
    "for optimizer, history in zip(optimizers, results):\n",
    "    plt.plot(history.history['val_loss'], label=optimizer)\n",
    "\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(optimizers, loc='upper right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1cf1c-faa8-4963-b59a-e9c429e320b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
