{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90e6186d-220b-4baa-8635-7f32e291f151",
   "metadata": {},
   "source": [
    "Q1. Difference Between Simple Linear Regression and Multiple Linear Regression\n",
    "Simple Linear Regression:\n",
    "\n",
    "Definition: A statistical method to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered the independent variable (predictor), and the other is the dependent variable (response).\n",
    "\n",
    "Equation: \n",
    "ğ‘¦\n",
    "=\n",
    "ğ‘\n",
    "0\n",
    "+\n",
    "ğ‘\n",
    "1\n",
    "ğ‘¥\n",
    "y=b \n",
    "0\n",
    "â€‹\n",
    " +b \n",
    "1\n",
    "â€‹\n",
    " x\n",
    "\n",
    "Example: Predicting a person's height (y) based on their shoe size (x).\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "x = np.array([5, 6, 7, 8, 9]).reshape(-1, 1)\n",
    "y = np.array([150, 160, 170, 180, 190])\n",
    "\n",
    "# Simple linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Definition: An extension of simple linear regression that uses multiple predictors to model the relationship with the dependent variable.\n",
    "\n",
    "Equation: \n",
    "ğ‘¦\n",
    "=\n",
    "ğ‘\n",
    "0\n",
    "+\n",
    "ğ‘\n",
    "1\n",
    "ğ‘¥\n",
    "1\n",
    "+\n",
    "ğ‘\n",
    "2\n",
    "ğ‘¥\n",
    "2\n",
    "+\n",
    "â‹¯\n",
    "+\n",
    "ğ‘\n",
    "ğ‘›\n",
    "ğ‘¥\n",
    "ğ‘›\n",
    "y=b \n",
    "0\n",
    "â€‹\n",
    " +b \n",
    "1\n",
    "â€‹\n",
    " x \n",
    "1\n",
    "â€‹\n",
    " +b \n",
    "2\n",
    "â€‹\n",
    " x \n",
    "2\n",
    "â€‹\n",
    " +â‹¯+b \n",
    "n\n",
    "â€‹\n",
    " x \n",
    "n\n",
    "â€‹\n",
    " \n",
    "\n",
    "Example: Predicting a person's weight (y) based on their height (x1) and age (x2).\n",
    "\n",
    "# Example data\n",
    "x = np.array([[5, 20], [6, 25], [7, 30], [8, 35], [9, 40]])\n",
    "y = np.array([55, 60, 65, 70, 75])\n",
    "\n",
    "# Multiple linear regression\n",
    "model = LinearRegression()\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f601ff-6ad7-449a-a3b0-cfb0fca109fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Assumptions of Linear Regression\n",
    "Linearity: The relationship between the predictors and the response is linear.\n",
    "Independence: Observations are independent of each other.\n",
    "Homoscedasticity: The residuals have constant variance at every level of the predictor.\n",
    "Normality: The residuals of the model are normally distributed.\n",
    "No multicollinearity: Predictors are not highly correlated with each other.\n",
    "\n",
    "Checking Assumptions:\n",
    "Linearity: Scatter plots and residual plots.\n",
    "Independence: Durbin-Watson test.\n",
    "Homoscedasticity: Residual vs. fitted values plot.\n",
    "Normality: Q-Q plot and Shapiro-Wilk test.\n",
    "Multicollinearity: Variance Inflation Factor (VIF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434d06d-c082-4b6d-9f14-cc5870fb2fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Interpreting the Slope and Intercept in a Linear Regression Model\n",
    "Intercept (b0): The expected value of the dependent variable when all predictors are zero. It represents the starting point of the model.\n",
    "Slope (b1): The change in the dependent variable for a one-unit change in the predictor variable, holding all other predictors constant.\n",
    "Example:\n",
    "Predicting a person's salary based on their years of experience.\n",
    "\n",
    "Intercept: The expected salary with 0 years of experience.\n",
    "Slope: The increase in salary for each additional year of experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f59fd9-8d87-49f8-9cf5-61b7ddca6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Concept of Gradient Descent in Machine Learning\n",
    "Gradient Descent: An optimization algorithm used to minimize the cost function in machine learning models, particularly linear regression.\n",
    "\n",
    "Process:\n",
    "\n",
    "Initialize the parameters (weights and biases) randomly.\n",
    "Calculate the gradient of the cost function with respect to each parameter.\n",
    "Update the parameters by subtracting the gradient scaled by a learning rate.\n",
    "Repeat until convergence (i.e., the change in the cost function is below a certain threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c898de-7905-4a7b-8e64-f0bd7228220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Multiple Linear Regression Model\n",
    "Definition: A regression model that uses multiple predictors to model the relationship with the dependent variable.\n",
    "\n",
    "Equation: \n",
    "ğ‘¦\n",
    "=\n",
    "ğ‘\n",
    "0\n",
    "+\n",
    "ğ‘\n",
    "1\n",
    "ğ‘¥\n",
    "1\n",
    "+\n",
    "ğ‘\n",
    "2\n",
    "ğ‘¥\n",
    "2\n",
    "+\n",
    "â‹¯\n",
    "+\n",
    "ğ‘\n",
    "ğ‘›\n",
    "ğ‘¥\n",
    "ğ‘›\n",
    "y=b \n",
    "0\n",
    "â€‹\n",
    " +b \n",
    "1\n",
    "â€‹\n",
    " x \n",
    "1\n",
    "â€‹\n",
    " +b \n",
    "2\n",
    "â€‹\n",
    " x \n",
    "2\n",
    "â€‹\n",
    " +â‹¯+b \n",
    "n\n",
    "â€‹\n",
    " x \n",
    "n\n",
    "â€‹\n",
    " \n",
    "\n",
    "Differences from Simple Linear Regression:\n",
    "\n",
    "Uses multiple predictors.\n",
    "Accounts for the combined effect of several variables on the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cb9add-2051-4b55-9873-c50c8d3f7517",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Multicollinearity in Multiple Linear Regression\n",
    "Definition: A situation where two or more predictors in the model are highly correlated, making it difficult to isolate the individual \n",
    "effect of each predictor.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Variance Inflation Factor (VIF): If VIF > 10, it indicates high multicollinearity.\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove highly correlated predictors.\n",
    "Use dimensionality reduction techniques like PCA.\n",
    "Regularization methods like Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ffb6b-b6b1-443e-867c-7b786fa32a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Polynomial Regression Model\n",
    "Definition: A type of regression model that fits a polynomial equation to the data. It can model non-linear relationships between the\n",
    "predictors and the response variable.\n",
    "\n",
    "Equation: \n",
    "ğ‘¦\n",
    "=\n",
    "ğ‘\n",
    "0\n",
    "+\n",
    "ğ‘\n",
    "1\n",
    "ğ‘¥\n",
    "+\n",
    "ğ‘\n",
    "2\n",
    "ğ‘¥\n",
    "2\n",
    "+\n",
    "â‹¯\n",
    "+\n",
    "ğ‘\n",
    "ğ‘›\n",
    "ğ‘¥\n",
    "ğ‘›\n",
    "y=b \n",
    "0\n",
    "â€‹\n",
    " +b \n",
    "1\n",
    "â€‹\n",
    " x+b \n",
    "2\n",
    "â€‹\n",
    " x \n",
    "2\n",
    " +â‹¯+b \n",
    "n\n",
    "â€‹\n",
    " x \n",
    "n\n",
    " \n",
    "\n",
    "Differences from Linear Regression:\n",
    "\n",
    "Polynomial regression can capture non-linear relationships.\n",
    "Linear regression fits a straight line, while polynomial regression fits a curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d0783-6806-4482-bafb-a39ef8fe3233",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Advantages and Disadvantages of Polynomial Regression\n",
    "Advantages:\n",
    "\n",
    "Can model non-linear relationships.\n",
    "More flexible compared to linear regression.\n",
    "Disadvantages:\n",
    "\n",
    "Prone to overfitting, especially with high-degree polynomials.\n",
    "Computationally more expensive.\n",
    "When to Use:\n",
    "\n",
    "When the relationship between the predictors and the response variable is non-linear.\n",
    "When the data shows curvature that cannot be captured by a linear model.\n",
    "Example of Polynomial Regression in Python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "x = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1, 4, 9, 16, 25])\n",
    "\n",
    "# Polynomial features\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "x_poly = poly.fit_transform(x)\n",
    "\n",
    "# Polynomial regression\n",
    "model = LinearRegression()\n",
    "model.fit(x_poly, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
