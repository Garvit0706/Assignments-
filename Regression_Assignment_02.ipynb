{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d2ddabf-7b1d-4896-a807-5b4dbafdbdbc",
   "metadata": {},
   "source": [
    "1. Concept of R-squared in Linear Regression Models\n",
    "R-squared (\n",
    "ğ‘…\n",
    "2\n",
    "R \n",
    "2\n",
    " ):\n",
    "\n",
    "Definition: R-squared is a statistical measure that represents the proportion of the variance for the dependent variable that's explained by the independent variables in a regression model.\n",
    "Calculation: \n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "1\n",
    "âˆ’\n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘Ÿ\n",
    "ğ‘’\n",
    "ğ‘ \n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘¡\n",
    "ğ‘œ\n",
    "ğ‘¡\n",
    "R \n",
    "2\n",
    " =1âˆ’ \n",
    "SS \n",
    "tot\n",
    "â€‹\n",
    " \n",
    "SS \n",
    "res\n",
    "â€‹\n",
    " \n",
    "â€‹\n",
    " \n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘Ÿ\n",
    "ğ‘’\n",
    "ğ‘ \n",
    "SS \n",
    "res\n",
    "â€‹\n",
    " : Sum of squares of residuals.\n",
    "ğ‘†\n",
    "ğ‘†\n",
    "ğ‘¡\n",
    "ğ‘œ\n",
    "ğ‘¡\n",
    "SS \n",
    "tot\n",
    "â€‹\n",
    " : Total sum of squares (variance of the dependent variable).\n",
    "Representation: R-squared values range from 0 to 1. A value closer to 1 indicates that a larger proportion of the variance is explained by the model.\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = r2_score(y, predictions)\n",
    "print(f\"R-squared: {r_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19359e55-5137-403f-8fe9-d23fec2884d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Definition of Adjusted R-squared and its Difference from Regular R-squared\n",
    "Adjusted R-squared:\n",
    "\n",
    "Definition: Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model. It accounts for the addition \n",
    "of variables to the model, which can artificially inflate the R-squared value.\n",
    "Calculation: \n",
    "AdjustedÂ \n",
    "ğ‘…\n",
    "2\n",
    "=\n",
    "1\n",
    "âˆ’\n",
    "(\n",
    "1\n",
    "âˆ’\n",
    "ğ‘…\n",
    "2\n",
    ")\n",
    "(\n",
    "ğ‘›\n",
    "âˆ’\n",
    "1\n",
    ")\n",
    "ğ‘›\n",
    "âˆ’\n",
    "ğ‘\n",
    "âˆ’\n",
    "1\n",
    "AdjustedÂ R \n",
    "2\n",
    " =1âˆ’ \n",
    "nâˆ’pâˆ’1\n",
    "(1âˆ’R \n",
    "2\n",
    " )(nâˆ’1)\n",
    "â€‹\n",
    " \n",
    "ğ‘›\n",
    "n: Number of observations.\n",
    "ğ‘\n",
    "p: Number of predictors.\n",
    "Difference: Unlike R-squared, which can only increase with more predictors, adjusted R-squared can decrease if the added predictors do not\n",
    "improve the model.\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Example adjusted R-squared calculation\n",
    "def adjusted_r2(r_squared, n, p):\n",
    "    return 1 - (1 - r_squared) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Parameters\n",
    "n = 10  # number of observations\n",
    "p = 2   # number of predictors\n",
    "r_squared = 0.9  # example R-squared\n",
    "\n",
    "# Calculate adjusted R-squared\n",
    "adj_r_squared = adjusted_r2(r_squared, n, p)\n",
    "print(f\"Adjusted R-squared: {adj_r_squared}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0d74e-ddf6-4cb1-af50-5ab03d51dd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. When to Use Adjusted R-squared\n",
    "When to Use: Adjusted R-squared is more appropriate when comparing models with different numbers of predictors. \n",
    "It penalizes the addition of predictors that do not improve the model significantly, preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da236753-d765-4f8e-b4b3-e9a32fa3b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. RMSE, MSE, and MAE in Regression Analysis\n",
    "Definitions:\n",
    "\n",
    "Mean Squared Error (MSE): The average of the squared differences between the actual and predicted values. \n",
    "MSE\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "(\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    ")\n",
    "2\n",
    "MSE= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " (y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " ) \n",
    "2\n",
    " \n",
    "Root Mean Squared Error (RMSE): The square root of the MSE. It has the same units as the dependent variable. \n",
    "RMSE\n",
    "=\n",
    "MSE\n",
    "RMSE= \n",
    "MSE\n",
    "â€‹\n",
    " \n",
    "Mean Absolute Error (MAE): The average of the absolute differences between the actual and predicted values. \n",
    "MAE\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ‘\n",
    "ğ‘–\n",
    "=\n",
    "1\n",
    "ğ‘›\n",
    "âˆ£\n",
    "ğ‘¦\n",
    "ğ‘–\n",
    "âˆ’\n",
    "ğ‘¦\n",
    "^\n",
    "ğ‘–\n",
    "âˆ£\n",
    "MAE= \n",
    "n\n",
    "1\n",
    "â€‹\n",
    " âˆ‘ \n",
    "i=1\n",
    "n\n",
    "â€‹\n",
    " âˆ£y \n",
    "i\n",
    "â€‹\n",
    " âˆ’ \n",
    "y\n",
    "^\n",
    "â€‹\n",
    "  \n",
    "i\n",
    "â€‹\n",
    " âˆ£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22638497-e1b6-433b-8d17-6c13e50f0ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Advantages and Disadvantages of RMSE, MSE, and MAE\n",
    "Advantages:\n",
    "\n",
    "MSE: Penalizes larger errors more than smaller ones due to squaring; useful for emphasizing large errors.\n",
    "RMSE: Has the same units as the dependent variable; easier to interpret compared to MSE.\n",
    "MAE: More robust to outliers; represents the average error in absolute terms.\n",
    "Disadvantages:\n",
    "\n",
    "MSE: Sensitive to outliers; can be difficult to interpret due to squared units.\n",
    "RMSE: Sensitive to outliers; can be misleading if not considering the context of the dependent variable.\n",
    "MAE: Does not penalize larger errors as much as MSE/RMSE; less sensitive to variance.\n",
    "Usage: Choose the metric based on the context of the problem and the importance of penalizing larger errors versus overall error robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9a427d-b3c9-457e-807a-ab2a283d7c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Lasso Regularization\n",
    "Lasso Regularization (Least Absolute Shrinkage and Selection Operator):\n",
    "\n",
    "Definition: A type of linear regression that includes a penalty equal to the absolute value of the magnitude of coefficients.\n",
    "Equation: \n",
    "Loss\n",
    "=\n",
    "RSS\n",
    "+\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "âˆ£\n",
    "ğ›½\n",
    "ğ‘—\n",
    "âˆ£\n",
    "Loss=RSS+Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " âˆ£Î² \n",
    "j\n",
    "â€‹\n",
    " âˆ£\n",
    "Difference from Ridge Regularization: Lasso uses \n",
    "ğ¿\n",
    "1\n",
    "L1 norm (absolute values), whereas Ridge uses \n",
    "ğ¿\n",
    "2\n",
    "L2 norm (squared values). Lasso can shrink some coefficients to zero, effectively performing feature selection.\n",
    "When to Use: When you have many features and expect that only a subset of them are useful for the model.\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Example data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
    "y = [1, 2, 3, 4]\n",
    "\n",
    "# Lasso regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "print(f\"Coefficients: {lasso.coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11660276-8afa-45e5-b2e9-66cce54253fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Regularized Linear Models and Overfitting\n",
    "Regularized Linear Models: Models that include a penalty term to reduce the magnitude of coefficients, thereby preventing overfitting.\n",
    "\n",
    "Ridge: Adds a penalty equal to the sum of the squared coefficients (\n",
    "ğ¿\n",
    "2\n",
    "L2 norm).\n",
    "Lasso: Adds a penalty equal to the sum of the absolute coefficients (\n",
    "ğ¿\n",
    "1\n",
    "L1 norm).\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ridge regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X, y)\n",
    "print(f\"Coefficients: {ridge.coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c12c3e-0957-4ba2-b89d-4cf57368c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Limitations of Regularized Linear Models\n",
    "Limitations:\n",
    "\n",
    "Bias-Variance Tradeoff: Regularization introduces bias, which can lead to underfitting if not balanced correctly.\n",
    "Feature Scaling: Requires proper feature scaling to work effectively.\n",
    "Not Always Optimal: May not capture complex, non-linear relationships in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6224b5e7-1e31-4f23-ace0-45fffa5b0086",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Comparing Regression Models with Different Evaluation Metrics\n",
    "Comparing RMSE and MAE:\n",
    "\n",
    "Model A: RMSE of 10.\n",
    "Model B: MAE of 8.\n",
    "Choosing the Better Model:\n",
    "\n",
    "If minimizing large errors is more important, prefer Model A (lower RMSE).\n",
    "If overall error robustness is more important, prefer Model B (lower MAE).\n",
    "Limitations: Different metrics capture different aspects of model performance.\n",
    "It's crucial to consider the context and objectives of the analysis when choosing the evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83588d4d-3764-4916-bc54-625ff7703968",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. Comparing Regularized Linear Models with Different Regularization Methods\n",
    "Comparing Ridge and Lasso:\n",
    "\n",
    "Model A (Ridge): Regularization parameter of 0.1.\n",
    "Model B (Lasso): Regularization parameter of 0.5.\n",
    "Choosing the Better Model:\n",
    "\n",
    "If feature selection is essential, prefer Model B (Lasso) as it can shrink coefficients to zero.\n",
    "If capturing all features with reduced magnitude is essential, prefer Model A (Ridge).\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Lasso: Effective for feature selection but can be less stable in high-dimensional data.\n",
    "Ridge: Better at handling multicollinearity but doesn't perform feature selection.\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "# Ridge and Lasso regression\n",
    "ridge = Ridge(alpha=0.1)\n",
    "lasso = Lasso(alpha=0.5)\n",
    "\n",
    "ridge.fit(X, y)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "print(f\"Ridge coefficients: {ridge.coef_}\")\n",
    "print(f\"Lasso coefficients: {lasso.coef_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
