{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8adc87-cfce-42be-a786-549428498ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Ridge Regression and Its Difference from Ordinary Least Squares Regression\n",
    "Ridge Regression:\n",
    "\n",
    "Definition: Ridge regression is a type of linear regression that includes a regularization term to penalize the size of the coefficients,\n",
    "effectively shrinking them towards zero. This helps prevent overfitting.\n",
    "Equation: \n",
    "Loss\n",
    "=\n",
    "RSS\n",
    "+\n",
    "ùúÜ\n",
    "‚àë\n",
    "ùëó\n",
    "=\n",
    "1\n",
    "ùëù\n",
    "ùõΩ\n",
    "ùëó\n",
    "2\n",
    "Loss=RSS+Œª‚àë \n",
    "j=1\n",
    "p\n",
    "‚Äã\n",
    " Œ≤ \n",
    "j\n",
    "2\n",
    "‚Äã\n",
    " \n",
    "RSS\n",
    "RSS: Residual sum of squares.\n",
    "ùúÜ\n",
    "Œª: Regularization parameter.\n",
    "ùõΩ\n",
    "ùëó\n",
    "Œ≤ \n",
    "j\n",
    "‚Äã\n",
    " : Coefficients.\n",
    "Difference from OLS: Ordinary least squares (OLS) regression minimizes the residual sum of squares without any penalty term.\n",
    "Ridge regression adds the penalty term to control for large coefficients, which can help in cases of multicollinearity or overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd422e7-a5dd-456e-9b17-63db6514fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Assumptions of Ridge Regression\n",
    "Assumptions:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables is linear.\n",
    "Independence: Observations are independent of each other.\n",
    "Homoscedasticity: Constant variance of the errors.\n",
    "Normality of errors: The errors of the model are normally distributed (important for inference).\n",
    "No perfect multicollinearity: While ridge regression can handle multicollinearity, it assumes no perfect multicollinearity\n",
    "(i.e., no perfect linear relationship between the predictors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e859eb5-2701-45f2-8715-0dfdb1dac019",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Selecting the Value of the Tuning Parameter (Lambda) in Ridge Regression\n",
    "Selecting \n",
    "ùúÜ\n",
    "Œª:\n",
    "\n",
    "Cross-Validation: A common method to select the optimal value of \n",
    "ùúÜ\n",
    "Œª is through k-fold cross-validation. The process involves partitioning the data into k subsets, \n",
    "training the model on k-1 subsets, and validating it on the remaining subset. This is repeated k times, \n",
    "and the average error across all k iterations is used to select the \n",
    "ùúÜ\n",
    "Œª that minimizes the error.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
    "y = [1, 2, 3, 4]\n",
    "\n",
    "# Define possible values of lambda\n",
    "alphas = np.logspace(-6, 6, 13)\n",
    "\n",
    "# Perform cross-validation\n",
    "ridge_cv = [cross_val_score(Ridge(alpha=a), X, y, cv=5).mean() for a in alphas]\n",
    "\n",
    "# Select the best lambda\n",
    "best_alpha = alphas[np.argmax(ridge_cv)]\n",
    "print(f\"Best alpha: {best_alpha}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adefdd26-fbdc-444f-9139-4cd077a7c6a2",
   "metadata": {},
   "source": [
    "Q4. Can Ridge Regression be Used for Feature Selection?\n",
    "Feature Selection:\n",
    "\n",
    "Ridge regression is generally not used for feature selection because it shrinks coefficients towards zero but does not set any coefficients exactly to zero. Instead, it is used to retain all features but with reduced impact from multicollinearity.\n",
    "Alternative for Feature Selection: Lasso regression (L1 regularization) is more appropriate for feature selection as it can shrink some coefficients to zero, effectively performing variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8930c952-2c15-4b90-8203-7cb33fcdf96f",
   "metadata": {},
   "source": [
    "Q5. Ridge Regression and Multicollinearity\n",
    "Performance in the Presence of Multicollinearity:\n",
    "\n",
    "Ridge regression performs well in the presence of multicollinearity because the regularization term penalizes large coefficients, thus stabilizing the estimates and reducing the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878aac80-4ffc-4bdc-a512-28a2edee9708",
   "metadata": {},
   "source": [
    "Q6. Handling Both Categorical and Continuous Independent Variables\n",
    "Handling Variables:\n",
    "\n",
    "Ridge regression can handle both categorical and continuous variables. However, categorical variables need to be properly encoded (e.g., using one-hot encoding) before being included in the model.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {'category': ['A', 'B', 'A', 'C'], 'value': [1, 2, 3, 4]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), ['category']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Define the ridge regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('ridge', ridge)])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(df[['category', 'value']], [1, 2, 3, 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001efe9f-85db-4e73-89e6-c71dbf08f9f8",
   "metadata": {},
   "source": [
    "Q7. Interpreting the Coefficients of Ridge Regression\n",
    "Interpreting Coefficients:\n",
    "\n",
    "The coefficients in ridge regression are interpreted similarly to those in OLS regression but with the understanding that they are shrunk towards zero due to the regularization term.\n",
    "A smaller coefficient indicates a weaker relationship between the predictor and the response variable, and this shrinkage is intended to reduce overfitting and improve model generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d544ab-17df-41db-9258-aeb9adb0860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Using Ridge Regression for Time-Series Data Analysis\n",
    "Ridge Regression for Time-Series:\n",
    "\n",
    "Ridge regression can be used for time-series data analysis, but it is essential to account for the temporal dependencies in the data.\n",
    "Approach:\n",
    "Include lagged variables as predictors.\n",
    "Ensure the model accounts for autocorrelation in the residuals, possibly by using techniques like time series cross-validation.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Example time series data\n",
    "data = {'value': np.sin(np.arange(0, 10, 0.1))}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create lagged features\n",
    "df['lag1'] = df['value'].shift(1)\n",
    "df['lag2'] = df['value'].shift(2)\n",
    "\n",
    "# Drop missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define predictors and response\n",
    "X = df[['lag1', 'lag2']]\n",
    "y = df['value']\n",
    "\n",
    "# Time series split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "    # Fit ridge regression\n",
    "    model = Ridge(alpha=1.0)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f\"Coefficients: {model.coef_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
