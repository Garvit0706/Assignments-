{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "016a057e-7ded-478e-9dc3-75e52e53a491",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression:\n",
    "\n",
    "Definition: Lasso regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that uses L1 regularization to penalize the absolute size of the coefficients.\n",
    "Equation:\n",
    "Loss\n",
    "=\n",
    "RSS\n",
    "+\n",
    "ùúÜ\n",
    "‚àë\n",
    "ùëó\n",
    "=\n",
    "1\n",
    "ùëù\n",
    "‚à£\n",
    "ùõΩ\n",
    "ùëó\n",
    "‚à£\n",
    "Loss=RSS+Œª \n",
    "j=1\n",
    "‚àë\n",
    "p\n",
    "‚Äã\n",
    " ‚à£Œ≤ \n",
    "j\n",
    "‚Äã\n",
    " ‚à£\n",
    "RSS\n",
    "RSS: Residual sum of squares.\n",
    "ùúÜ\n",
    "Œª: Regularization parameter.\n",
    "ùõΩ\n",
    "ùëó\n",
    "Œ≤ \n",
    "j\n",
    "‚Äã\n",
    " : Coefficients.\n",
    "Difference: Lasso regression differs from ordinary least squares (OLS) regression by adding a penalty term that can shrink some coefficients to exactly zero, thus performing variable selection. It also differs from Ridge regression (L2 regularization), which shrinks coefficients but does not set any to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7455c32f-2f02-41ea-bcf0-7b5bf610b969",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "Main Advantage:\n",
    "\n",
    "Feature Selection: Lasso regression can shrink some coefficients to exactly zero, effectively removing those features from the model. This makes Lasso useful for feature selection, especially in cases where there are many predictors and a subset of them are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18be7a-218f-45b0-ad85-c1ffc1a2f49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Interpreting Coefficients:\n",
    "\n",
    "Non-Zero Coefficients: Coefficients that are non-zero indicate that the corresponding features are important for the model.\n",
    "Zero Coefficients: Coefficients that are exactly zero indicate that the corresponding features have been excluded from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b078d-ebc0-4ed4-88ba-b81e2337780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "Tuning Parameters:\n",
    "\n",
    "ùúÜ\n",
    "Œª: The main tuning parameter in Lasso regression. It controls the strength of the regularization.\n",
    "Effect:\n",
    "A large \n",
    "ùúÜ\n",
    "Œª will increase regularization, potentially setting more coefficients to zero (more feature selection but possibly higher bias).\n",
    "A small \n",
    "ùúÜ\n",
    "Œª will decrease regularization, leading to a model closer to OLS regression (less feature selection but potentially lower bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27d98b-0c05-4d2e-ac27-f99fca323aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Non-Linear Regression:\n",
    "\n",
    "Lasso regression is inherently a linear model. However, it can handle non-linear relationships if the data is transformed appropriately\n",
    "(e.g., using polynomial features or basis expansions).\n",
    "Example:\n",
    "Polynomial Features: Create polynomial features from the original features and then apply Lasso regression.\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4]]\n",
    "y = [2, 4, 6, 8]\n",
    "\n",
    "# Define the pipeline\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2)),\n",
    "    ('lasso', Lasso(alpha=0.1))\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284a758-053d-4354-b568-58dcaa4dbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "Difference:\n",
    "\n",
    "Ridge Regression (L2 regularization):\n",
    "Adds a penalty equal to the sum of the squared values of the coefficients.\n",
    "Tends to shrink coefficients towards zero but does not set any coefficients exactly to zero.\n",
    "Good for handling multicollinearity but does not perform feature selection.\n",
    "\n",
    "Lasso Regression (L1 regularization):\n",
    "Adds a penalty equal to the sum of the absolute values of the coefficients.\n",
    "Can shrink some coefficients to exactly zero, thus performing feature selection.\n",
    "Useful when only a subset of predictors are believed to be relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d391c53-2feb-4e38-a04a-8fa763a6faa6",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Handling Multicollinearity:\n",
    "\n",
    "Yes, Lasso regression can handle multicollinearity.\n",
    "Mechanism: By shrinking some coefficients to zero, Lasso effectively reduces the number of predictors, which can help mitigate the issues associated with multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa1500f-ba56-4d21-9bc5-1f72cb74ac7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "Choosing \n",
    "ùúÜ\n",
    "Œª:\n",
    "\n",
    "Cross-Validation: A common approach to select the optimal value of \n",
    "ùúÜ\n",
    "Œª is through k-fold cross-validation.\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Example data\n",
    "X = [[1, 2], [3, 4], [5, 6], [7, 8]]\n",
    "y = [1, 2, 3, 4]\n",
    "\n",
    "# LassoCV automatically selects the best alpha\n",
    "model = LassoCV(cv=5)\n",
    "model.fit(X, y)\n",
    "\n",
    "print(f\"Optimal alpha: {model.alpha_}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
